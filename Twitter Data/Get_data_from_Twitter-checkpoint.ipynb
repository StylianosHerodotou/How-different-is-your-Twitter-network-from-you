{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "from pymongo.errors import WriteError\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "import math\n",
    "import random\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.read_csv(\"/home/ubuntu/Desktop/DATASET_Crunchbase_Founders_with_Twitter v2.csv\", delimiter=\"\\t\")\n",
    "# list_of_people = data[\"twitter_username\"]\n",
    "#first\n",
    "consumer_key1 = \"\"\n",
    "consumer_secret_key1 = \"\"\n",
    "access_tokken1 = \"\"\n",
    "access_tokken_secret1 = \"\"\n",
    "\n",
    "#second staff\n",
    "consumer_key2 =\"\"\n",
    "consumer_secret_key2=\"\"\n",
    "access_tokken2=\"-\"\n",
    "access_tokken_secret2=\"\"\n",
    "# # pass twitter credentials to tweepy\n",
    "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "# auth.set_access_token(access_tokken, access_tokken_secret)\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "#                  wait_on_rate_limit_notify=True,\n",
    "#                  parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "\n",
    "def generate_random_sample(full_list, starting, final, sample_size):\n",
    "    # generate random sample\n",
    "    if(final>len(full_list)):\n",
    "        final=len(full_list)\n",
    "    if(sample_size>len(full_list)):\n",
    "        s=len(full_list)\n",
    "    else:\n",
    "        s=sample_size\n",
    "    random_sample_indexes = generate_n_uniqe_random_integers(starting=starting, final=final, n=s)\n",
    "    random_sample = []\n",
    "    for index in random_sample_indexes:\n",
    "        random_sample.append(full_list[index])\n",
    "    return random_sample\n",
    "\n",
    "\n",
    "def get_everything_to_a_list(collection, start_from_beggining=False,per_time=200):\n",
    "    flag = False\n",
    "    everything=[]\n",
    "    count = collection.estimated_document_count()\n",
    "    if(count==0):\n",
    "        return []\n",
    "    if(start_from_beggining==True):\n",
    "        count=0\n",
    "    print(count)\n",
    "\n",
    "    while (flag != True):\n",
    "        new_staff = collection.find({}).skip(count).limit(per_time)\n",
    "        if (new_staff == None):\n",
    "            flag = True\n",
    "            break\n",
    "        try:\n",
    "            new_staff = list(new_staff)\n",
    "            print(new_staff)\n",
    "            if(len(new_staff)==0):\n",
    "                flag=True\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "        count = count + len(new_staff)\n",
    "        everything = everything+new_staff\n",
    "    return everything\n",
    "\n",
    "\n",
    "def generate_n_uniqe_random_integers(starting: int = 0, final: int = -1,\n",
    "                                     n=10):\n",
    "    ans = random.sample(range(starting, final), n)\n",
    "    return ans\n",
    "\n",
    "#\n",
    "\n",
    "def process_these_tweets(tweets):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        processed_tweet = process_this_tweet(tweet)\n",
    "        processed_tweets.append(processed_tweet)\n",
    "    return processed_tweets\n",
    "\n",
    "\n",
    "def process_this_tweet(tweet):\n",
    "    status = tweet\n",
    "    status_dict = dict(status)\n",
    "    # ta keys p en eshi apefthias pio kato.\n",
    "    user_id = status_dict[\"user\"][\"id\"]\n",
    "    user = status_dict[\"user\"][\"screen_name\"]\n",
    "    original_text = status_dict[\"text\"]\n",
    "    clean_text = clean_tweet(original_text)\n",
    "    # find polarity and subjectiviy\n",
    "    blob = TextBlob(clean_text)\n",
    "    Sentiment = blob.sentiment\n",
    "    polarity = Sentiment.polarity\n",
    "    subjectivity = Sentiment.subjectivity\n",
    "    # find hashtags\n",
    "    hashtags = []\n",
    "    lod_for_hashtags = status_dict[\"entities\"][\"hashtags\"]\n",
    "    for dic in lod_for_hashtags:\n",
    "        hashtags.append(dic[\"text\"])\n",
    "    # find user_mentions\n",
    "    user_mentions = []\n",
    "    lod_for_user_mentions = status_dict[\"entities\"][\"user_mentions\"]\n",
    "    for dic in lod_for_user_mentions:\n",
    "        user_mentions.append(dic[\"screen_name\"])\n",
    "    # find sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(original_text)\n",
    "    sentiment = vs[\"compound\"]  # mono tuto valo ala maybe ena theli je ta alla 3. tra na dume.\n",
    "    positive = vs[\"pos\"]\n",
    "    negative = vs[\"neg\"]\n",
    "    neutral = vs[\"neu\"]\n",
    "    # find id\n",
    "    inti = status_dict[\"id\"]\n",
    "    is_quote = status_dict[\"is_quote_status\"]\n",
    "    retweet_count = status_dict[\"retweet_count\"]\n",
    "    favorite_count = status_dict[\"favorite_count\"]\n",
    "    is_reply = None\n",
    "    if (status_dict[\"in_reply_to_screen_name\"] != None):\n",
    "        is_reply = True\n",
    "    else:\n",
    "        is_reply = False\n",
    "\n",
    "    # neg=negative, neu=neutral , pos=positive compound vasika\n",
    "    # positive sentiment: compound score >= 0.05\n",
    "    # neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    # negative sentiment: compound score <= -0.05\n",
    "    # -1 (most extreme negative) and +1 (most extreme positive).\n",
    "    # me to for vali ta columns p en mesti lista colums.\n",
    "    ans = {\"_id\": inti,\n",
    "           \"user\": user,\n",
    "           \"user_id\": user_id,\n",
    "           \"original_text\": original_text,\n",
    "           \"clean_text\": clean_text,\n",
    "           \"polarity\": polarity,\n",
    "           \"subjectivity\": subjectivity,\n",
    "           \"hashtag_count\": len(hashtags),\n",
    "           \"user_mention_count\": len(user_mentions),\n",
    "           \"sentiment\": sentiment,\n",
    "           \"is_quote\": is_quote,\n",
    "           \"retweet_count\": retweet_count,\n",
    "           \"favorite_count\": favorite_count,\n",
    "           \"is_reply\": is_reply\n",
    "\n",
    "           }\n",
    "    return ans\n",
    "\n",
    "def process_and_add_tweets_to_database(unpne_tweets_col,processed_tweets_collection,tweets_os_tora=0):\n",
    "    flag=False\n",
    "    count_of_tweets=processed_tweets_collection.count()\n",
    "    while(flag!=True):\n",
    "        unpne_tweets=unpne_tweets_col.find({}).skip(count_of_tweets).limit(200)\n",
    "        if(unpne_tweets==None):\n",
    "            flag=True\n",
    "            break\n",
    "        try:\n",
    "            unpne_tweets=list(unpne_tweets)\n",
    "        except:\n",
    "            continue\n",
    "        count_of_tweets=count_of_tweets+len(unpne_tweets)\n",
    "        processed_tweets=process_these_tweets(unpne_tweets)\n",
    "        try:\n",
    "            processed_tweets_collection.insert_many(processed_tweets)\n",
    "        except:\n",
    "            print(\"i could not add all of them\")\n",
    "\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix[x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix[0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x - 1] == seq2[y - 1]:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x - 1, y] + 1,\n",
    "                    matrix[x - 1, y - 1],\n",
    "                    matrix[x, y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x - 1, y] + 1,\n",
    "                    matrix[x - 1, y - 1] + 1,\n",
    "                    matrix[x, y - 1] + 1\n",
    "                )\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "\n",
    "def get_current_date():\n",
    "    temp = datetime.today().strftime('%d-%m-%Y')\n",
    "    temp = temp.split(\"-\")\n",
    "    ans_date = (int(temp[0]), int(temp[1]), int(temp[2]))\n",
    "    return ans_date\n",
    "\n",
    "\n",
    "def get_days_so_far(today, then):\n",
    "    d_today = date(today[2], today[1], today[0])\n",
    "    d_then = date(then[2], then[1], then[0])\n",
    "    days = d_today - d_then\n",
    "    return days.days\n",
    "\n",
    "\n",
    "def find_number_of_posts_per_week(number_of_posts, created_at):\n",
    "    then = find_date(created_at)\n",
    "    today = get_current_date()\n",
    "    days_between = get_days_so_far(today, then)\n",
    "    ans = number_of_posts / days_between\n",
    "    return ans\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = {':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "                 '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('}\n",
    "# HappyEmoticons\n",
    "emoticons_happy = {':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D',\n",
    "                   '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P',\n",
    "                   ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'}\n",
    "# Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "\n",
    "def find_a_count(text):\n",
    "    lista = text.split()\n",
    "    count = 0\n",
    "    for word in lista:\n",
    "        if (word.startswith(\"@\") and (len(word) > 1)):\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def removeContractions(text: str) -> str:\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    lista = []\n",
    "    keep = True\n",
    "    for w in w_tokenizer.tokenize((text)):\n",
    "        if (w == \"?\"):\n",
    "            keep = False\n",
    "        elif (keep == False):\n",
    "            keep = True\n",
    "        else:\n",
    "            lista.append(w)\n",
    "    #  print(len(lista))\n",
    "    ans = ' '.join(lista)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    return ' '.join(word.strip(string.punctuation) for word in words.split())\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    lista = []\n",
    "    for w in w_tokenizer.tokenize((text)):\n",
    "        lista.append(lemmatizer.lemmatize(w, get_wordnet_pos(w)))\n",
    "    ans = ' '.join(lista)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    return set(part[1:] for part in s.split() if part.startswith('#'))\n",
    "\n",
    "\n",
    "def clean_description(dirty: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # gia na eshi ta new lines\n",
    "    tweet = dirty.replace('\\n', ' ').replace('\\r', '')\n",
    "    # Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #     print(\"lower\",tweet)\n",
    "    # removes punctuation\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    #     print(\"punctuation\",tweet)\n",
    "    # remove contractions\n",
    "    tweet = removeContractions(tweet)\n",
    "    #     print(\"contractions\",tweet)\n",
    "    # clean this shit\n",
    "    tweet = p.clean(tweet)\n",
    "    #     print(\"cleaned it\",tweet)\n",
    "    # after tweepy preprocessing the colon symbol left remain after removing mentions\n",
    "\n",
    "    # replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    #     print(\"ascii char\",tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    #     print(tweet)\n",
    "    tweet = re.sub(r'Ä¶', '', tweet)\n",
    "    #     print(tweet)\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    # print(\"emojies\",tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "    # print(\"remove numbers\",tweet)\n",
    "\n",
    "    # lemmatize text\n",
    "    tweet = lemmatize_text(tweet)\n",
    "    # print(\"lem\",tweet)\n",
    "\n",
    "    # token word\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    # remove stopwords\n",
    "    clean = []\n",
    "    w_t = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words:\n",
    "            w_t.append(i)\n",
    "    word_tokens = w_t\n",
    "\n",
    "    #  print(\"stop words\",' '.join(word_tokens))\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons:\n",
    "            clean.append(w)\n",
    "    return ' '.join(clean)\n",
    "\n",
    "\n",
    "def process_description(description: str) -> str:\n",
    "    original_text = description\n",
    "    clean_text = clean_description(original_text)\n",
    "    # find polarity and subjectiviy\n",
    "    blob = TextBlob(clean_text)\n",
    "    Sentiment = blob.sentiment\n",
    "    polarity = Sentiment.polarity\n",
    "    subjectivity = Sentiment.subjectivity\n",
    "    # find hashtags\n",
    "    hashtag_count = len(extract_hash_tags(original_text))\n",
    "    # find user_mentions\n",
    "    user_mention_count = find_a_count(original_text)\n",
    "    # find sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(original_text)\n",
    "    sentiment = vs[\"compound\"]  # mono tuto valo ala maybe ena theli je ta alla 3. tra na dume.\n",
    "    positive = vs[\"pos\"]\n",
    "    negative = vs[\"neg\"]\n",
    "    neutral = vs[\"neu\"]\n",
    "\n",
    "    # neg=negative, neu=neutral , pos=positive compound vasika\n",
    "    # positive sentiment: compound score >= 0.05\n",
    "    # neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    # negative sentiment: compound score <= -0.05\n",
    "    # -1 (most extreme negative) and +1 (most extreme positive).\n",
    "    # me to for vali ta columns p en mesti lista colums.\n",
    "    discreption_dict = {\n",
    "        \"polarity\": polarity,\n",
    "        \"subjectivity\": subjectivity,\n",
    "        \"hashtag_count\": hashtag_count,\n",
    "        \"user_mention_count\": user_mention_count,\n",
    "        \"sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "    return discreption_dict\n",
    "\n",
    "\n",
    "def get_avg_sentiment_of_user(user_id,collection):\n",
    "    all_tweeets_of_user = collection.find({\"user\": {\"id\": user_id}})\n",
    "    count = 0\n",
    "    for i, tweet in enumerate(all_tweeets_of_user):\n",
    "        sum_sentiment = sum_sentiment + tweet[\"sentiment\"]\n",
    "        count = i + 1\n",
    "\n",
    "    return sum_sentiment / count\n",
    "\n",
    "\n",
    "def find_followers_following_ratio(follower_count, following_count):\n",
    "    return follower_count / following_count\n",
    "\n",
    "\n",
    "def find_date(date_str: str):\n",
    "    months = {'Jan': 1,\n",
    "              \"Feb\": 2,\n",
    "              \"Mar\": 3,\n",
    "              \"Apr\": 4,\n",
    "              \"May\": 5,\n",
    "              \"Jun\": 6,\n",
    "              \"Jul\": 7,\n",
    "              \"Aug\": 8,\n",
    "              \"Sep\": 9,\n",
    "              \"Oct\": 10,\n",
    "              \"Nov\": 11,\n",
    "              \"Dec\": 12}\n",
    "\n",
    "    splitted = date_str.split()\n",
    "    month = months[splitted[1]]\n",
    "    day = splitted[2]\n",
    "    year = splitted[-1]\n",
    "    try:\n",
    "        ans_date = (int(day), month, int(year))\n",
    "    except:\n",
    "        ans_date = None\n",
    "    return ans_date\n",
    "\n",
    "\n",
    "def find_number_of_posts_per_week(number_of_posts, created_at):\n",
    "    then = find_date(created_at)\n",
    "    today = get_current_date()\n",
    "    days_between = get_days_so_far(today, then)\n",
    "    ans = number_of_posts / days_between\n",
    "    return ans\n",
    "\n",
    "\n",
    "def clean_tweet(dirty: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # gia na eshi ta new lines\n",
    "    tweet = dirty.replace('\\n', ' ').replace('\\r', '')\n",
    "    # Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #     print(\"lower\",tweet)\n",
    "    # removes punctuation\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    #     print(\"punctuation\",tweet)\n",
    "    # remove contractions\n",
    "    tweet = removeContractions(tweet)\n",
    "    #     print(\"contractions\",tweet)\n",
    "    # clean this shit\n",
    "    tweet = p.clean(tweet)\n",
    "    #     print(\"cleaned it\",tweet)\n",
    "    # after tweepy preprocessing the colon symbol left remain after removing mentions\n",
    "\n",
    "    # replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    #     print(\"ascii char\",tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    #     print(tweet)\n",
    "    tweet = re.sub(r'Ä¶', '', tweet)\n",
    "    #     print(tweet)\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    # print(\"emojies\",tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "    # print(\"remove numbers\",tweet)\n",
    "\n",
    "    # lemmatize text\n",
    "    tweet = lemmatize_text(tweet)\n",
    "    # print(\"lem\",tweet)\n",
    "\n",
    "    # token word\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    # remove stopwords\n",
    "    clean = []\n",
    "    w_t = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words:\n",
    "            w_t.append(i)\n",
    "    word_tokens = w_t\n",
    "\n",
    "    #  print(\"stop words\",' '.join(word_tokens))\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons:\n",
    "            clean.append(w)\n",
    "    return ' '.join(clean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_and_add_tweets_of_users(sample_of_friends,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "                                     keep_track_of_tweets_collection,max_tweets_per_user,code_for_api):\n",
    "    api=None\n",
    "    if(code_for_api==1):\n",
    "        auth = tweepy.OAuthHandler(consumer_key1, consumer_secret_key1)\n",
    "        auth.set_access_token(access_tokken1, access_tokken_secret1)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                         wait_on_rate_limit_notify=True,\n",
    "                         parser=tweepy.parsers.JSONParser())\n",
    "    else:\n",
    "        auth = tweepy.OAuthHandler(consumer_key2, consumer_secret_key2)\n",
    "        auth.set_access_token(access_tokken2, access_tokken_secret2)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                         wait_on_rate_limit_notify=True,\n",
    "                         parser=tweepy.parsers.JSONParser())\n",
    "    for friend_id in sample_of_friends:\n",
    "        find_and_add_tweets_of_user(user_id=friend_id, unprocessed=unprocessed_tweets_of_friends_collection, processed=processed_tweets_of_friends_collection,\n",
    "                                        keep_track_of_tweets_collection=keep_track_of_tweets_collection,api=api,max_tweets_per_user=max_tweets_per_user)\n",
    "\n",
    "\n",
    "def find_and_add_tweets_of_user(user_id: int, unprocessed, processed, keep_track_of_tweets_collection,api,\n",
    "                                    max_tweets_per_user=3240):\n",
    "    # Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    temp = keep_track_of_tweets_collection.find_one({\"_id\": user_id})\n",
    "\n",
    "    if (temp == None):\n",
    "        all_tweet_count = 0\n",
    "    else:\n",
    "        all_tweet_count = temp[\"count\"]\n",
    "\n",
    "    # initialize a list to hold all the tweepy Tweets and the tweet data, soon to be the df\n",
    "    tweets_data = []\n",
    "    all_tweets = []\n",
    "\n",
    "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    try:\n",
    "        if (temp == None or temp[\"oldest\"] == -1):\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200)\n",
    "        else:\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=temp[\"oldest\"])\n",
    "    except:\n",
    "        print(\"unable to get tweets for user id \", user_id)\n",
    "        return tweets_data\n",
    "    all_tweets = all_tweets + new_tweets\n",
    "    all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "    # process the first tweets and add them to the tweets_data\n",
    "    processed_tweets = process_these_tweets(new_tweets)\n",
    "    tweets_data = tweets_data + processed_tweets\n",
    "    for tweet in new_tweets:\n",
    "        tweet[\"_id\"] = tweet.pop(\"id\")\n",
    "\n",
    "    oldest = -1\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "    if (len(new_tweets) > 0):\n",
    "        oldest = new_tweets[-1][\"_id\"] - 1\n",
    "\n",
    "    try:\n",
    "        if (len(new_tweets) > 0):\n",
    "            unprocessed.insert_many(new_tweets)\n",
    "        if (len(processed_tweets) > 0):\n",
    "            processed.insert_many(processed_tweets)\n",
    "    except:\n",
    "        print(\"i was unable to save evrything to the database\")\n",
    "\n",
    "    # keep grabbing tweets until there are no tweets left to grab\n",
    "    while ((len(new_tweets) > 0) and all_tweet_count < max_tweets_per_user) > 0:\n",
    "        print(f\"getting tweets before {oldest}\")\n",
    "        # all subsiquent requests use the max_id param to prevent duplicates\n",
    "        try:\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=oldest)\n",
    "        except:\n",
    "            print(\"unable to get the rest of the tweets for user id  \", user_id)\n",
    "            break\n",
    "        all_tweets = all_tweets + new_tweets\n",
    "        all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "        # process the tweets and add them to the tweets_data\n",
    "        processed_tweets = process_these_tweets(new_tweets)\n",
    "        tweets_data = tweets_data + processed_tweets\n",
    "        for tweet in new_tweets:\n",
    "            tweet[\"_id\"] = tweet.pop(\"id\")\n",
    "        if (len(new_tweets) > 0):\n",
    "            oldest = new_tweets[-1][\"_id\"] - 1\n",
    "        try:\n",
    "            if (len(new_tweets) > 0):\n",
    "                unprocessed.insert_many(new_tweets)\n",
    "                # update the id of the oldest tweet less one\n",
    "            if (len(processed_tweets) > 0):\n",
    "                processed.insert_many(processed_tweets)\n",
    "        except:\n",
    "            print(\"i was unable to save evrything to the database\")\n",
    "\n",
    "        print(f\"...{all_tweet_count} tweets downloaded so far\")\n",
    "    if (temp == None):\n",
    "        q = {\n",
    "            \"_id\": user_id,\n",
    "            \"count\": all_tweet_count,\n",
    "            \"oldest\": oldest\n",
    "        }\n",
    "        try:\n",
    "            keep_track_of_tweets_collection.insert_one(q)\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            print(\"attempted dublicate entry\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            keep_track_of_tweets_collection.update_one({\"_id\": user_id},\n",
    "                                                       {\"$set\": {\"count\": all_tweet_count, \"oldest\": oldest}})\n",
    "        except:\n",
    "            print(\"i was unable to update the keep_track_of_tweets_collection \")\n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "\n",
    "def find_and_add_friends_of_user(entrepreneur_id, sample_of_friends, friend_collection,\n",
    "                          relationship_collection, code_for_friendship,code_for_api):\n",
    "    api=None\n",
    "    if(code_for_api==1):\n",
    "        auth = tweepy.OAuthHandler(consumer_key1, consumer_secret_key1)\n",
    "        auth.set_access_token(access_tokken1, access_tokken_secret1)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                         wait_on_rate_limit_notify=True,\n",
    "                         parser=tweepy.parsers.JSONParser())\n",
    "    else:\n",
    "        auth = tweepy.OAuthHandler(consumer_key2, consumer_secret_key2)\n",
    "        auth.set_access_token(access_tokken2, access_tokken_secret2)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                         wait_on_rate_limit_notify=True,\n",
    "                         parser=tweepy.parsers.JSONParser())\n",
    "    for friends_id in sample_of_friends:\n",
    "        # check an en idi mesa\n",
    "        find_and_add_friend_of_user(friends_id=friends_id,entrepreneur_id=entrepreneur_id,friend_collection=friend_collection,relationship_collection=relationship_collection,code=code_for_friendship,api=api)\n",
    "\n",
    "def find_and_add_friend_of_user(friends_id, entrepreneur_id, friend_collection,\n",
    "                                     relationship_collection, code, api):\n",
    "    # arxika elenxi oti den exo idi get_user afto ton user\n",
    "    if (friend_collection.find_one({\"_id\": friends_id}) == None):\n",
    "        try:\n",
    "            # an oxi katevazo to xristi\n",
    "            friend = api.get_user(user_id=friends_id)\n",
    "            friend = dict(friend)\n",
    "            friend[\"_id\"] = friend.pop(\"id\")\n",
    "            try:\n",
    "                # prospatho na ton valo sto database\n",
    "                friend_collection.insert_one(friend)\n",
    "            except WriteError as e:\n",
    "                print(\"i could insert the user in database \", friends_id)\n",
    "                return\n",
    "        except tweepy.error.TweepError as e:\n",
    "            print(\"didnt find this user \", friends_id)\n",
    "            return\n",
    "    else:\n",
    "        print(\"this user is already in the database \", friends_id)\n",
    "    # ite iparxi idi ite oxi dimiourgo ti sxesi me afto ton entrepreneur\n",
    "    temp=None\n",
    "    friend_id=friends_id\n",
    "    if (code == 1):\n",
    "        relationship = {\n",
    "            \"_id\": {\n",
    "                \"is_followed\": entrepreneur_id,\n",
    "                \"is_followed_by\": friend_id\n",
    "                    }\n",
    "                }\n",
    "        temp=relationship_collection.find_one({\"_id\": {\n",
    "                \"is_followed\": entrepreneur_id,\n",
    "                \"is_followed_by\": friend_id\n",
    "            }})\n",
    "    else:\n",
    "        relationship = {\n",
    "            \"_id\": {\n",
    "                \"is_followed\": friend_id,\n",
    "                \"is_followed_by\": entrepreneur_id\n",
    "                    }\n",
    "                }\n",
    "        temp=relationship_collection.find_one({\"_id\": {\n",
    "                \"is_followed\": friend_id,\n",
    "                \"is_followed_by\": entrepreneur_id\n",
    "            }})\n",
    "    # elenxo ean iparxi idi i sxesi\n",
    "    if (temp != None):\n",
    "        print(\"relationship already exists\")\n",
    "    else:\n",
    "        try:\n",
    "            # ti prostheto sto db\n",
    "            relationship_collection.insert_one(relationship)\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            \n",
    "            \n",
    "            # ean den katafero na ti valo vgazo\n",
    "            print(\"this relationship is already in databse\")\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            print(\"there was a problem inserting reletionship\")\n",
    "\n",
    "\n",
    "\n",
    "def create_entrepreneurs_processes(entrepreneurs,followers_collection,following_collection,relationship_collection,\n",
    "             unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection,max_tweets_per_user,\n",
    "             max_friends_per_user,starting_index, finishing_index):\n",
    "\n",
    "\n",
    "    index=starting_index\n",
    "    pool = multiprocessing.Pool() #use all available cores, otherwise specify the number you want as an argument\n",
    "    while(index<finishing_index):\n",
    "        entrepreneur=entrepreneurs[index]\n",
    "\n",
    "        random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "        random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "        arguments=(entrepreneur,random_sample_followers,random_sample_following,followers_collection,following_collection,relationship_collection,\n",
    "             unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection,max_tweets_per_user)\n",
    "        pool.apply_async(create_entrepreneur_process, args=arguments)\n",
    "        index=index+1\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "\n",
    "def create_entrepreneur_process(entrepreneur,random_sample_followers,random_sample_following,followers_collection,following_collection,relationship_collection,\n",
    "             unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection,max_tweets_per_user):\n",
    "    # code=1->followers\n",
    "    code_for_followers = 1\n",
    "    # code=2->Followings\n",
    "    code_for_followings = 2\n",
    "    #code_for_api1\n",
    "    code_for_api_1=1\n",
    "    #code_for_api2\n",
    "    code_for_api_2=2\n",
    "    print(code_for_followers)\n",
    "\n",
    "\n",
    "    args1 = (random_sample_followers, unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection, max_tweets_per_user, code_for_api_1)\n",
    "    args2 = (entrepreneur[\"_id\"], random_sample_followers, followers_collection, relationship_collection, code_for_followers,\n",
    "    code_for_api_1)\n",
    "    args3 = (random_sample_following, unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection, max_tweets_per_user, code_for_api_2)\n",
    "    args4 = (entrepreneur[\"_id\"], random_sample_following, following_collection, relationship_collection, code_for_followings,\n",
    "    code_for_api_2)\n",
    "\n",
    "    pool = multiprocessing.Pool()  # use all available cores, otherwise specify the number you want as an argument\n",
    "    pool.apply_async(find_and_add_tweets_of_users, args=args1)\n",
    "    pool.apply_async(find_and_add_friends_of_user, args=args2)\n",
    "    pool.apply_async(find_and_add_tweets_of_users, args=args3)\n",
    "    pool.apply_async(find_and_add_friends_of_user, args=args4)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# def main():\n",
    "#     client = MongoClient('localhost', 27017)\n",
    "#     db = client[\"testDB\"]\n",
    "#     followers_collection=db[\"Followers\"]\n",
    "#     following_collection=db[\"Following\"]\n",
    "#     relationship_collection=db[\"relationships\"]\n",
    "#     processed_tweets_of_friends_collection = db[\"processed_tweets_friends_and_followers\"]\n",
    "#     unprocessed_tweets_of_friends_collection = db[\"unprocessed_tweets_friends_and_followers\"]\n",
    "#     final_sample_collection = db[\"final_sample\"]\n",
    "#     keep_track_of_tweets_collection = db[\"keep_track_of_tweets\"]\n",
    "\n",
    "#     entrepreneurs = get_everything_to_a_list(final_sample_collection, True)\n",
    "#     starting_index=0\n",
    "#     finishing_index=len(entrepreneurs)\n",
    "#     max_tweets_per_user = 600\n",
    "#     max_friends_per_user = 150\n",
    "#     print(len(entrepreneurs))\n",
    "\n",
    "# #     create_entrepreneurs_processes(entrepreneurs, followers_collection, following_collection, relationship_collection,\n",
    "# #                                    unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection,\n",
    "# #                                    keep_track_of_tweets_collection, max_tweets_per_user,\n",
    "# #                                    max_friends_per_user, starting_index, finishing_index)\n",
    "\n",
    "#     #turn_screen_name_to_lowercase(collection)\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def mix(entrepreneurs,followers_collection,following_collection,relationship_collection,\n",
    "#              unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "#              keep_track_of_tweets_collection,max_tweets_per_user,\n",
    "#              max_friends_per_user,starting_index, finishing_index):\n",
    "#     # code=1->followers\n",
    "#     code_for_followers = 1\n",
    "#     # code=2->Followings\n",
    "#     code_for_followings = 2\n",
    "#     #code_for_api1\n",
    "#     code_for_api_1=1\n",
    "#     #code_for_api2\n",
    "#     code_for_api_2=2\n",
    "#\n",
    "#     index=starting_index\n",
    "#     while(index<finishing_index):\n",
    "#         entrepreneur=entrepreneurs[index]\n",
    "#\n",
    "#         random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "#         random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "#\n",
    "#         args1=(random_sample_followers,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_1)\n",
    "#         args2=(entrepreneur[\"_id\"],random_sample_followers,followers_collection,relationship_collection,code_for_followers,code_for_api_1)\n",
    "#         args3=(random_sample_following,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_2)\n",
    "#         args4=(entrepreneur[\"_id\"],random_sample_following,following_collection,relationship_collection,code_for_followings,code_for_api_2)\n",
    "#\n",
    "#\n",
    "#         p1 = Process(target=find_and_add_tweets_of_users,args=args1)\n",
    "#         p2 = Process(target=find_and_add_friends_of_user,args=args2)\n",
    "#         p3 = Process(target=find_and_add_tweets_of_users,args=args3)\n",
    "#         p4 = Process(target=find_and_add_friends_of_user,args=args4)\n",
    "#\n",
    "#         p1.start()\n",
    "#         p2.start()\n",
    "#         p3.start()\n",
    "#         p4.start()\n",
    "#\n",
    "#         p1.join()\n",
    "#         p2.join()\n",
    "#         p3.join()\n",
    "#         p4.join()\n",
    "#\n",
    "#         index=index+1\n",
    "#\n",
    "\n",
    "# def get_all_friends_users(entrepreneurs, friend_collection,\n",
    "#                           relationship_collection, max_friends_per_user, code):\n",
    "#     # code=1->followers\n",
    "#     # code=2->followings\n",
    "#     for entrepreneur in entrepreneurs:\n",
    "#         if (code == 1):\n",
    "#             friends_ids = entrepreneur[\"followers_ids\"]\n",
    "#         else:\n",
    "#             friends_ids = entrepreneur[\"following_ids\"]\n",
    "#\n",
    "#         random_sample = generate_random_sample(friends_ids,\n",
    "#                                                0, len(friends_ids), max_friends_per_user)\n",
    "#\n",
    "#         for friends_id in random_sample:\n",
    "#             # check an en idi mesa\n",
    "#             if (friend_collection.find_one({\"_id\": friends_id}) == None):\n",
    "#                 try:\n",
    "#                     friend = api.get_user(user_id=friends_id)\n",
    "#                     friend = dict(friend)\n",
    "#                     friend[\"_id\"] = friend.pop(\"id\")\n",
    "#                     try:\n",
    "#                         friend_collection.insert_one(friend)\n",
    "#                     except WriteError as e:\n",
    "#                         print(\"i could insert the user in database \", friends_id)\n",
    "#                     if (code == 1):\n",
    "#                         relationship = {\n",
    "#                             \"is_followed\": entrepreneur[\"_id\"],\n",
    "#                             \"is_followed_by\": friend[\"_id\"]\n",
    "#                         }\n",
    "#                     else:\n",
    "#                         relationship = {\n",
    "#                             \"is_followed\": friend[\"_id\"],\n",
    "#                             \"is_followed_by\": entrepreneur[\"_id\"]\n",
    "#                         }\n",
    "#                     try:\n",
    "#                         relationship_collection.insert_one(relationship)\n",
    "#                     except pymongo.errors.DuplicateKeyError:\n",
    "#                         print(\"this relationship is already in databse\")\n",
    "#                         print(\"removing following\", friend[\"_id\"])\n",
    "#                         relationship_collection.remove({\"_id\": friend[\"_id\"]})\n",
    "#                     except pymongo.errors.DuplicateKeyError:\n",
    "#                         print(\"there was a problem inserting reletionship\")\n",
    "#                         print(\"removing following\", friend[\"_id\"])\n",
    "#                         relationship_collection.remove({\"_id\": friend[\"_id\"]})\n",
    "#\n",
    "#                 except tweepy.error.TweepError as e:\n",
    "#                     print(\"didnt find this user \", friends_id)\n",
    "#             else:\n",
    "#                 print(\"this user is already in the database \", friends_id)\n",
    "#                 continue\n",
    "\n",
    "# def find_and_add_all_tweets_of_user(user_id: int, unprocessed, processed, keep_track_of_tweets_collection,\n",
    "#                                     max_tweets_per_user=3240):\n",
    "#     # Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "#\n",
    "#     temp = keep_track_of_tweets_collection.find_one({\"_id\": user_id})\n",
    "#\n",
    "#     if (temp == None):\n",
    "#         all_tweet_count = 0\n",
    "#     else:\n",
    "#         all_tweet_count = temp[\"count\"]\n",
    "#\n",
    "#     # initialize a list to hold all the tweepy Tweets and the tweet data, soon to be the df\n",
    "#     tweets_data = []\n",
    "#     all_tweets = []\n",
    "#\n",
    "#     # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "#     try:\n",
    "#         if (temp == None or temp[\"oldest\"] == -1):\n",
    "#             new_tweets = api.user_timeline(user_id=user_id, count=200)\n",
    "#         else:\n",
    "#             new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=temp[\"oldest\"])\n",
    "#     except:\n",
    "#         print(\"unable to get tweets for user id \", user_id)\n",
    "#         return tweets_data\n",
    "#     all_tweets = all_tweets + new_tweets\n",
    "#     all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "#     # process the first tweets and add them to the tweets_data\n",
    "#     processed_tweets = process_these_tweets(new_tweets)\n",
    "#     tweets_data = tweets_data + processed_tweets\n",
    "#     for tweet in new_tweets:\n",
    "#         tweet[\"_id\"] = tweet.pop(\"id\")\n",
    "#\n",
    "#     oldest = -1\n",
    "#\n",
    "#     # save the id of the oldest tweet less one\n",
    "#     if (len(new_tweets) > 0):\n",
    "#         oldest = new_tweets[-1][\"_id\"] - 1\n",
    "#\n",
    "#     try:\n",
    "#         if (len(new_tweets) > 0):\n",
    "#             unprocessed.insert_many(new_tweets)\n",
    "#         if (len(processed_tweets) > 0):\n",
    "#             processed.insert_many(processed_tweets)\n",
    "#     except:\n",
    "#         print(\"i was unable to save evrything to the database\")\n",
    "#\n",
    "#     # keep grabbing tweets until there are no tweets left to grab\n",
    "#     while ((len(new_tweets) > 0) and all_tweet_count < max_tweets_per_user) > 0:\n",
    "#         print(f\"getting tweets before {oldest}\")\n",
    "#         # all subsiquent requests use the max_id param to prevent duplicates\n",
    "#         try:\n",
    "#             new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=oldest)\n",
    "#         except:\n",
    "#             print(\"unable to get the rest of the tweets for user id  \", user_id)\n",
    "#             break\n",
    "#         all_tweets = all_tweets + new_tweets\n",
    "#         all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "#         # process the tweets and add them to the tweets_data\n",
    "#         processed_tweets = process_these_tweets(new_tweets)\n",
    "#         tweets_data = tweets_data + processed_tweets\n",
    "#         for tweet in new_tweets:\n",
    "#             tweet[\"_id\"] = tweet.pop(\"id\")\n",
    "#         if (len(new_tweets) > 0):\n",
    "#             oldest = new_tweets[-1][\"_id\"] - 1\n",
    "#         try:\n",
    "#             if (len(new_tweets) > 0):\n",
    "#                 unprocessed.insert_many(new_tweets)\n",
    "#                 # update the id of the oldest tweet less one\n",
    "#             if (len(processed_tweets) > 0):\n",
    "#                 processed.insert_many(processed_tweets)\n",
    "#         except:\n",
    "#             print(\"i was unable to save evrything to the database\")\n",
    "#\n",
    "#         print(f\"...{all_tweet_count} tweets downloaded so far\")\n",
    "#     if (temp == None):\n",
    "#         q = {\n",
    "#             \"_id\": user_id,\n",
    "#             \"count\": all_tweet_count,\n",
    "#             \"oldest\": oldest\n",
    "#         }\n",
    "#         try:\n",
    "#             keep_track_of_tweets_collection.insert_one(q)\n",
    "#         except pymongo.errors.DuplicateKeyError:\n",
    "#             print(\"attempted dublicate entry\")\n",
    "#\n",
    "#     else:\n",
    "#         try:\n",
    "#             keep_track_of_tweets_collection.update_one({\"_id\": user_id},\n",
    "#                                                        {\"$set\": {\"count\": all_tweet_count, \"oldest\": oldest}})\n",
    "#         except:\n",
    "#             print(\"i was unable to update the keep_track_of_tweets_collection \")\n",
    "#     return tweets_data\n",
    "#\n",
    "# def get_all_tweets_from_not_entrepreneurs(entrepreneurs,unprocessed,processed,final_sample_collection,\n",
    "#         keep_track_of_tweets_collection,max_tweets_per_user=3240,max_friends_per_user=5000):\n",
    "#     misoi=math.ceil(len(entrepreneurs) / 2)\n",
    "#     for i,entrepreneur in enumerate(entrepreneurs):\n",
    "#\n",
    "#         if(i>=misoi):\n",
    "#             print(\"i>msoi \",i)\n",
    "#             break;\n",
    "#\n",
    "#         random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "#         random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "#\n",
    "#         for follower_id in random_sample_followers:\n",
    "#             find_and_add_all_tweets_of_user(follower_id,unprocessed,processed,keep_track_of_tweets_collection,max_tweets_per_user)\n",
    "#         for following_id in random_sample_following:\n",
    "#             find_and_add_all_tweets_of_user(following_id,unprocessed,processed,keep_track_of_tweets_collection,max_tweets_per_user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"testDB\"]\n",
    "followers_collection=db[\"w_followers\"]\n",
    "following_collection=db[\"w_followings\"]\n",
    "relationship_collection=db[\"relationships\"]\n",
    "processed_tweets_of_friends_collection = db[\"processed_tweets_friends_and_followers\"]\n",
    "unprocessed_tweets_of_friends_collection = db[\"unprocessed_tweets_friends_and_followers\"]\n",
    "final_sample_collection = db[\"final_sample\"]\n",
    "keep_track_of_tweets_collection = db[\"keep_track_of_tweets\"]\n",
    "\n",
    "entrepreneurs = get_everything_to_a_list(final_sample_collection, True)\n",
    "starting_index=0\n",
    "finishing_index=len(entrepreneurs)\n",
    "max_tweets_per_user = 600\n",
    "max_friends_per_user = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entrepreneurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mix(entrepreneurs,followers_collection,following_collection,relationship_collection,\n",
    "             unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection,max_tweets_per_user,\n",
    "             max_friends_per_user,starting_index, finishing_index):\n",
    "    # code=1->followers\n",
    "    code_for_followers = 1\n",
    "    # code=2->Followings\n",
    "    code_for_followings = 2\n",
    "    #code_for_api1\n",
    "    code_for_api_1=1\n",
    "    #code_for_api2\n",
    "    code_for_api_2=2\n",
    "\n",
    "    index=starting_index\n",
    "    while(index<finishing_index):\n",
    "        entrepreneur=entrepreneurs[index]\n",
    "\n",
    "        random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "        random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "\n",
    "        args1=(random_sample_followers,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_1)\n",
    "        args2=(entrepreneur[\"_id\"],random_sample_followers,followers_collection,relationship_collection,code_for_followers,code_for_api_1)\n",
    "        args3=(random_sample_following,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_2)\n",
    "        args4=(entrepreneur[\"_id\"],random_sample_following,following_collection,relationship_collection,code_for_followings,code_for_api_2)\n",
    "\n",
    "\n",
    "        p1 = Process(target=find_and_add_tweets_of_users,args=args1)\n",
    "        p2 = Process(target=find_and_add_friends_of_user,args=args2)\n",
    "        p3 = Process(target=find_and_add_tweets_of_users,args=args3)\n",
    "        p4 = Process(target=find_and_add_friends_of_user,args=args4)\n",
    "\n",
    "        p1.start()\n",
    "        p2.start()\n",
    "        p3.start()\n",
    "        p4.start()\n",
    "\n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "\n",
    "        index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/pymongo/topology.py:155: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: http://api.mongodb.org/python/current/faq.html#is-pymongo-fork-safe\n",
      "  \"MongoClient opened before fork. Create MongoClient only \"\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/pymongo/topology.py:155: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: http://api.mongodb.org/python/current/faq.html#is-pymongo-fork-safe\n",
      "  \"MongoClient opened before fork. Create MongoClient only \"\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/pymongo/topology.py:155: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: http://api.mongodb.org/python/current/faq.html#is-pymongo-fork-safe\n",
      "  \"MongoClient opened before fork. Create MongoClient only \"\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/pymongo/topology.py:155: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: http://api.mongodb.org/python/current/faq.html#is-pymongo-fork-safe\n",
      "  \"MongoClient opened before fork. Create MongoClient only \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was unable to save evrything to the database\n",
      "getting tweets before 644364371590647807\n",
      "...6 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-36:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to get tweets for user id unable to get tweets for user id  705445321581592579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-34:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 951362244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b66ac3ccb367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                \u001b[0munprocessed_tweets_of_friends_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_tweets_of_friends_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mkeep_track_of_tweets_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tweets_per_user\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                max_friends_per_user, starting_index, finishing_index)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-64afa134397e>\u001b[0m in \u001b[0;36mmix\u001b[0;34m(entrepreneurs, followers_collection, following_collection, relationship_collection, unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection, keep_track_of_tweets_collection, max_tweets_per_user, max_friends_per_user, starting_index, finishing_index)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mp4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mp3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-36-001ff4840d18>\", line 620, in find_and_add_friends_of_user\n",
      "    find_and_add_friend_of_user(friends_id=friends_id,entrepreneur_id=entrepreneur_id,friend_collection=friend_collection,relationship_collection=relationship_collection,code=code_for_friendship,api=api)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-36-001ff4840d18>\", line 628, in find_and_add_friend_of_user\n",
      "    friend = api.get_user(user_id=friends_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tweepy/binder.py\", line 250, in _call\n",
      "    return method.execute()\n",
      "  File \"<ipython-input-36-001ff4840d18>\", line 620, in find_and_add_friends_of_user\n",
      "    find_and_add_friend_of_user(friends_id=friends_id,entrepreneur_id=entrepreneur_id,friend_collection=friend_collection,relationship_collection=relationship_collection,code=code_for_friendship,api=api)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tweepy/binder.py\", line 189, in execute\n",
      "    proxies=self.api.proxy)\n",
      "  File \"<ipython-input-36-001ff4840d18>\", line 628, in find_and_add_friend_of_user\n",
      "    friend = api.get_user(user_id=friends_id)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tweepy/binder.py\", line 250, in _call\n",
      "    return method.execute()\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/tweepy/binder.py\", line 189, in execute\n",
      "    proxies=self.api.proxy)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 667, in urlopen\n",
      "    self._prepare_proxy(conn)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 930, in _prepare_proxy\n",
      "    conn.connect()\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connection.py\", line 370, in connect\n",
      "    ssl_context=context,\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 426, in _make_request\n",
      "    six.raise_from(e, None)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/util/ssl_.py\", line 377, in ssl_wrap_socket\n",
      "    return context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 407, in wrap_socket\n",
      "    _context=self, _session=session)\n",
      "  File \"/home/ubuntu/.local/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 817, in __init__\n",
      "    self.do_handshake()\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 1077, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 1364, in getresponse\n",
      "    response.begin()\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 689, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 307, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/usr/lib/python3.6/http/client.py\", line 268, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/usr/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 1012, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 874, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "  File \"/usr/lib/python3.6/ssl.py\", line 631, in read\n",
      "    v = self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was unable to save evrything to the database\n",
      "getting tweets before 58031000768356351\n",
      "...237 tweets downloaded so far\n",
      "getting tweets before 1281138326171312129\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 281810407147073535\n",
      "...433 tweets downloaded so far\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1278003251711897600\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 582240241521598463\n",
      "...210 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1264403984187486207\n",
      "...3 tweets downloaded so far\n",
      "unable to get tweets for user id  968042001554911232\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 840830899205885952\n",
      "...54 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1230485243213340672\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 215697792985858049\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1138450952908877824\n",
      "...579 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 450080789171273727\n",
      "...246 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 774957900443836415\n",
      "...600 tweets downloaded so far\n",
      "...30 tweets downloaded so far\n",
      "getting tweets before 1280857645239291903\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 10226458115\n",
      "...400 tweets downloaded so far\n",
      "\n",
      "getting tweets before 1276508174174961664...12 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 800957010715889663\n",
      "...6 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 9872446994\n",
      "...600 tweets downloaded so far\n",
      "...375 tweets downloaded so far\n",
      "getting tweets before 1245291697195499519\n",
      "i was unable to save evrything to the database\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1197903165313757195\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1153611028640366592\n",
      "...561 tweets downloaded so far\n",
      "unable to get tweets for user id  957713538025381888\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 651771459895881727\n",
      "...66 tweets downloaded so far\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 912911022104641536\n",
      "...39 tweets downloaded so far\n",
      "getting tweets before 1241275490695954431\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 440212740892684287\n",
      "...3 tweets downloaded so far\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1195253229300269055\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1146057427592339455\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 364995916438052863\n",
      "...84 tweets downloaded so far\n",
      "...798 tweets downloaded so far\n",
      "unable to get tweets for user id  126897014\n",
      "getting tweets before 12226379720\n",
      "...242 tweets downloaded so far\n",
      "getting tweets before 4409957281\n",
      "...242 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 432695718680481791\n",
      "...516 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 661859893335339007\n",
      "...150 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 914789006180737024\n",
      "...453 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 603447508673564672\n",
      "...48 tweets downloaded so far\n",
      "getting tweets before 1270954058686140415\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1255510156445659135\n",
      "i was unable to save evrything to the database\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 13473691078\n",
      "...360 tweets downloaded so far\n",
      "i was unable to save evrything to the databasegetting tweets before 706345320649588735\n",
      "\n",
      "...42 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 903855989740462079\n",
      "...9 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 885849359199330306\n",
      "...3 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 484716254591086591\n",
      "...6 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 882233481278898175\n",
      "...40 tweets downloaded so far\n",
      "getting tweets before 1237355455925702656\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 10258434541\n",
      "...27 tweets downloaded so far\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1215310624395276293\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 959488608506757120\n",
      "...3 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 784656762582527999\n",
      "...417 tweets downloaded so far\n",
      "getting tweets before 1281535544300670976\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1278355708509958146\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1276470400638214149\n",
      "i was unable to save evrything to the database\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1270603260781449215\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 542303093635813377\n",
      "...3 tweets downloaded so far\n",
      "getting tweets before 1239468788585607171\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 850921447786774527\n",
      "...54 tweets downloaded so far\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1196777734451712002\n",
      "i was unable to save evrything to the database\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1134704834513235973\n",
      "i was unable to save evrything to the database\n",
      "unable to get tweets for user id  293986653\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 804770992698560511\n",
      "...18 tweets downloaded so far\n",
      "...798 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 892364578725793791\n",
      "...3 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 669491709202423807\n",
      "...6 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "unable to get tweets for user id  69863777\n",
      "getting tweets before 1284364559663263744\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1284122980511965184\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 625199527704682495\n",
      "...288 tweets downloaded so far\n",
      "getting tweets before 1113263936911040512\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1068499766181543935\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 172513540878508033\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 21224432992\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 14426017732\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 1220048482318331909\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1155958064182284289\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1113492892100681727\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1264438415132626943\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1252113022006505473\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1241635663658676223\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1056441477126123519\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 981559714193575936\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1279485729333272576\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1274227059368198144\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283791516725473279\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1283066497544290306\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1281799910266789889\n",
      "...796 tweets downloaded so far\n",
      "getting tweets before 1275282753198514175\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1267752982684200959\n",
      "...600 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1174102481816735744\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1071908926898274303\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1270454339514118148\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1257746890633986047\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1184348311312056319\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1131439633236209663\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 950326734972338175\n",
      "...113 tweets downloaded so far\n",
      "getting tweets before 1278979220442472447\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1273618253508603903\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 471553834754400255\n",
      "...394 tweets downloaded so far\n",
      "getting tweets before 458521404896325631\n",
      "...587 tweets downloaded so far\n",
      "getting tweets before 408784653877657599\n",
      "...782 tweets downloaded so far\n",
      "getting tweets before 1283736088578678787\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1281599724156448778\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1280774193508790272\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 581024029575761919\n",
      "...45 tweets downloaded so far\n",
      "getting tweets before 1038260815256801279\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 962287562697134081\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 656894040319791103\n",
      "...351 tweets downloaded so far\n",
      "getting tweets before 600324682190053375\n",
      "...351 tweets downloaded so far\n",
      "getting tweets before 1279405909064560640\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1275718554441207809\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1275802016502222848\n",
      "...400 tweets downloaded so far\n",
      "\n",
      "getting tweets before 1268671207144132610...600 tweets downloaded so far\n",
      "getting tweets before 1266017054559100929\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1237363094139154431\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 7339394658729984\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 16179556025\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 5911971777\n",
      "...683 tweets downloaded so far\n",
      "getting tweets before 1278777727017943039\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1272788288491778048\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1263895401393840127\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 794201510150283263\n",
      "...252 tweets downloaded so far\n",
      "getting tweets before 791048743802011647\n",
      "...252 tweets downloaded so far\n",
      "getting tweets before 1226730191512145920\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1176545961352552448\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1069509344276897792\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1284391202880511999\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1284154598874165247\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1261666184459620352\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1247255826231734273\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 418960346842292224\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 153047812395905024\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 123044779645865983\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1214882250690629631\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1111084961111523328\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283009479986933759\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1281838802961494016\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1272968644947922943\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1263162368051212287\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1255191806276325375\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1115628833162649602\n",
      "...400 tweets downloaded so far\n",
      "\n",
      "getting tweets before 1059861747358748671...600 tweets downloaded so far\n",
      "getting tweets before 516626519515348992\n",
      "...354 tweets downloaded so far\n",
      "getting tweets before 324128029561462784\n",
      "...354 tweets downloaded so far\n",
      "getting tweets before 1280802795365347328\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1276805333684379647\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1279424790542106629\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1274187560529457152\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 604459091931455487\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 588371680512245760\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 551383543704608767\n",
      "...795 tweets downloaded so far\n",
      "getting tweets before 1283298785574236160\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1282154186071736319\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1262987819468783615\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1240671313426931712\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1268986488332537855\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1258379271795625983\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1260194936986767359\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1247752463077105667\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1240241046073692159\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1283388942977572865\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1282151969948409855\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283816525057720320\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283097004458029055\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1276508106567176191\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1270370268544937985\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1235985201940983807\n",
      "...400 tweets downloaded so far\n",
      "\n",
      "getting tweets before 1155832828308811775...600 tweets downloaded so far\n",
      "getting tweets before 1273465097524936703\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1265642254317699077\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1190577473530060799\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 1182115439830126591\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 1157880836932292608\n",
      "...796 tweets downloaded so far\n",
      "getting tweets before 1192327762398212096\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1143105576764764159\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 787913118945046528\n",
      "...261 tweets downloaded so far\n",
      "getting tweets before 571729794875826176\n",
      "...261 tweets downloaded so far\n",
      "getting tweets before 840315119372455935\n",
      "...394 tweets downloaded so far\n",
      "getting tweets before 646358519386632191\n",
      "...591 tweets downloaded so far\n",
      "getting tweets before 502588136745336833\n",
      "...790 tweets downloaded so far\n",
      "getting tweets before 1277439961365700607\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1272930457689174015\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1122111672842379266\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1087570587650686976\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1003173873561374720\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 1278348057692774402\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1272589035651969024\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1280102358450487296\n",
      "...400 tweets downloaded so far\n",
      "\n",
      "getting tweets before 1275051339114938367...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1275307607046021124\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1268581403593265153\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1222398933512536063\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1101419588154089471\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 969082950716452863\n",
      "...686 tweets downloaded so far\n",
      "getting tweets before 1251080701920727039\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1193003127999352831\n",
      "...600 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "mix(entrepreneurs, followers_collection, following_collection, relationship_collection,\n",
    "                               unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection,\n",
    "                               keep_track_of_tweets_collection, max_tweets_per_user,\n",
    "                               max_friends_per_user, starting_index, finishing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mix_2(entrepreneurs,followers_collection,following_collection,relationship_collection,\n",
    "             unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,\n",
    "             keep_track_of_tweets_collection,max_tweets_per_user,\n",
    "             max_friends_per_user,starting_index, finishing_index):\n",
    "    # code=1->followers\n",
    "    code_for_followers = 1\n",
    "    # code=2->Followings\n",
    "    code_for_followings = 2\n",
    "    #code_for_api1\n",
    "    code_for_api_1=1\n",
    "    #code_for_api2\n",
    "    code_for_api_2=2\n",
    "\n",
    "    index=starting_index\n",
    "    while(index<finishing_index):\n",
    "        entrepreneur=entrepreneurs[index]\n",
    "\n",
    "        random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "        random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "\n",
    "        find_and_add_tweets_of_users(random_sample_followers,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_1)\n",
    "        find_and_add_friends_of_user(entrepreneur[\"_id\"],random_sample_followers,followers_collection,relationship_collection,code_for_followers,code_for_api_1)\n",
    "        find_and_add_tweets_of_users(random_sample_following,unprocessed_tweets_of_friends_collection,processed_tweets_of_friends_collection,keep_track_of_tweets_collection,max_tweets_per_user,code_for_api_2)\n",
    "        find_and_add_friends_of_user(entrepreneur[\"_id\"],random_sample_following,following_collection,relationship_collection,code_for_followings,code_for_api_2)\n",
    "\n",
    "        index=index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was unable to save evrything to the database\n",
      "getting tweets before 549623383009136639\n",
      "...60 tweets downloaded so far\n",
      "unable to get tweets for user id  126897014\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 804770992698560511\n",
      "...24 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 203018606798315520\n",
      "...5 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 154331730944344063\n",
      "...70 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 881022442231156735\n",
      "...12 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1251182225464233986\n",
      "...16 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1266955809612877829\n",
      "...16 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 697106314263015423\n",
      "...28 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 10226458115\n",
      "...16 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 669491709202423807\n",
      "...8 tweets downloaded so far\n",
      "unable to get tweets for user id  69863777\n",
      "i was unable to save evrything to the database\n",
      "unable to get tweets for user id  957713538025381888\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 644364371590647807\n",
      "...10 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 661859893335339007\n",
      "...200 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 139986905411485696\n",
      "...12 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 436045465834897407\n",
      "...320 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 882233481278898175\n",
      "...50 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 9671795957\n",
      "...232 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 668467850047197184\n",
      "...80 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 542303093635813377\n",
      "...4 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 712710201707601919\n",
      "...4 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 180467185628622847\n",
      "...532 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 397228229242540031\n",
      "...165 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 625199527704682495\n",
      "...384 tweets downloaded so far\n",
      "unable to get tweets for user id  293986653\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 243256251365076992\n",
      "...590 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 901647607587414015\n",
      "...196 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 484716254591086591\n",
      "...8 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 10258434541\n",
      "...45 tweets downloaded so far\n",
      "unable to get tweets for user id  968042001554911232\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 840607789709967360\n",
      "...36 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1264403984187486207\n",
      "...5 tweets downloaded so far\n",
      "this user is already in the database  2182438333\n",
      "relationship already exists\n",
      "this user is already in the database  140810096\n",
      "relationship already exists\n",
      "this user is already in the database  4266610873\n",
      "relationship already exists\n",
      "this user is already in the database  2328196453\n",
      "relationship already exists\n",
      "this user is already in the database  149023897\n",
      "relationship already exists\n",
      "this user is already in the database  2518211354\n",
      "relationship already exists\n",
      "this user is already in the database  705445321581592579\n",
      "relationship already exists\n",
      "this user is already in the database  2711154703\n",
      "relationship already exists\n",
      "this user is already in the database  728354743\n",
      "relationship already exists\n",
      "this user is already in the database  833957277073817604\n",
      "relationship already exists\n",
      "this user is already in the database  2814838922\n",
      "relationship already exists\n",
      "this user is already in the database  914485788498264064\n",
      "relationship already exists\n",
      "this user is already in the database  247700867\n",
      "relationship already exists\n",
      "this user is already in the database  1968711854\n",
      "this user is already in the database  68119671\n",
      "relationship already exists\n",
      "this user is already in the database  4919793511\n",
      "relationship already exists\n",
      "this user is already in the database  281370739\n",
      "relationship already exists\n",
      "this user is already in the database  531301777\n",
      "relationship already exists\n",
      "this user is already in the database  402444359\n",
      "relationship already exists\n",
      "this user is already in the database  3115776643\n",
      "relationship already exists\n",
      "this user is already in the database  524952395\n",
      "relationship already exists\n",
      "this user is already in the database  211087828\n",
      "relationship already exists\n",
      "this user is already in the database  3977309420\n",
      "relationship already exists\n",
      "this user is already in the database  147925580\n",
      "relationship already exists\n",
      "this user is already in the database  1855072290\n",
      "relationship already exists\n",
      "this user is already in the database  1021169570\n",
      "relationship already exists\n",
      "this user is already in the database  845344228473524224\n",
      "relationship already exists\n",
      "this user is already in the database  306319492\n",
      "relationship already exists\n",
      "this user is already in the database  885847693859225602\n",
      "this user is already in the database  968042001554911232\n",
      "relationship already exists\n",
      "this user is already in the database  2459281062\n",
      "relationship already exists\n",
      "getting tweets before 1123587142783913983\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1112289511248146432\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 844972404648693759\n",
      "...10 tweets downloaded so far\n",
      "unable to get tweets for user id  69863777\n",
      "getting tweets before 1266972270708187135\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1244566599056494592\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1212983346898165759\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1185520548454862848\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 1160537544355770368\n",
      "...796 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1190577473530060799\n",
      "i was unable to save evrything to the database\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 1182115439830126591\n",
      "i was unable to save evrything to the database\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 1157880836932292608\n",
      "i was unable to save evrything to the database\n",
      "...796 tweets downloaded so far\n",
      "attempted dublicate entry\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 1280036733401419775\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1273525397053833215\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1237632043212816383\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1221379985195388928\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1186542349561876479\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 585124395305304063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...192 tweets downloaded so far\n",
      "getting tweets before 3752270387\n",
      "...52 tweets downloaded so far\n",
      "getting tweets before 1258999002919342079\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1244868054917382143\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1271432390653558785\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1263863098978586625\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1257195163153100800\n",
      "...796 tweets downloaded so far\n",
      "getting tweets before 1094938054760947711\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 989401362977968127\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 852405053742886911\n",
      "...608 tweets downloaded so far\n",
      "getting tweets before 11153480264\n",
      "...5 tweets downloaded so far\n",
      "getting tweets before 1103433104587350015\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 925959399276814335\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 832669130142670847\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1265753394116952073\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1258826385965101055\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1277249830411046911\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1092277444504018943\n",
      "...44 tweets downloaded so far\n",
      "getting tweets before 1278976848269983748\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1274233713610551300\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1110877177468334081\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 999304707603656703\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 990115406059798527\n",
      "...2 tweets downloaded so far\n",
      "getting tweets before 1252012746582851584\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1205994911574527999\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1229571620739657727\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1190607671386353663\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 1125749051000066047\n",
      "...794 tweets downloaded so far\n",
      "getting tweets before 1166516121618374655\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1095547614962036735\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 963017573649534975\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1255409628462157826\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1224179105509199872\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1156066256908214271\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1280329394934841343\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1269766348931911680\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1242456287444529151\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1194899978759618559\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1260778613319675904\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1257398985486196737\n",
      "...600 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "mix_2(entrepreneurs, followers_collection, following_collection, relationship_collection,\n",
    "                               unprocessed_tweets_of_friends_collection, processed_tweets_of_friends_collection,\n",
    "                               keep_track_of_tweets_collection, max_tweets_per_user,\n",
    "                               max_friends_per_user, starting_index, finishing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
