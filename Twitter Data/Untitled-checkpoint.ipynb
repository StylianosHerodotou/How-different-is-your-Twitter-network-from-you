{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy\n",
    "!pip install pandas\n",
    "!pip install textblob\n",
    "!pip install tweet-preprocessor\n",
    "!pip install nltk\n",
    "!pip install vaderSentiment\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv(\"/home/ubuntu/Desktop/DATASET_Crunchbase_Founders_with_Twitter v2.csv\", delimiter=\"\\t\")\n",
    "# list_of_people = data[\"twitter_username\"]\n",
    "consumer_key = \"\"\n",
    "consumer_secret_key = \"\"\n",
    "access_tokken = \""\n",
    "access_tokken_secret = \""\n",
    "# pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "auth.set_access_token(access_tokken, access_tokken_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                 wait_on_rate_limit_notify=True,\n",
    "                 parser=tweepy.parsers.JSONParser())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_random_sample(full_list, starting, final, sample_size):\n",
    "    # generate random sample\n",
    "    if(final>len(full_list)):\n",
    "        final=len(full_list)\n",
    "    if(sample_size>len(full_list)):\n",
    "        s=len(full_list)\n",
    "    else:\n",
    "        s=sample_size\n",
    "    random_sample_indexes = generate_n_uniqe_random_integers(starting=starting, final=final, n=s)\n",
    "    random_sample = []\n",
    "    for index in random_sample_indexes:\n",
    "        random_sample.append(full_list[index])\n",
    "    return random_sample\n",
    "\n",
    "def get_all_tweets_from_not_entrepreneurs(entrepreneurs,unprocessed,processed,final_sample_collection,\n",
    "        keep_track_of_tweets_collection,max_tweets_per_user=3240,max_friends_per_user=5000):\n",
    "    misoi=math.ceil(len(entrepreneurs) / 2)\n",
    "    for i,entrepreneur in enumerate(entrepreneurs):\n",
    "\n",
    "        if(i>=misoi):\n",
    "            print(\"i>msoi \",i)\n",
    "            break;\n",
    "\n",
    "        random_sample_followers=generate_random_sample(entrepreneur[\"followers_ids\"],0,len(entrepreneur[\"followers_ids\"]),max_friends_per_user)\n",
    "        random_sample_following = generate_random_sample(entrepreneur[\"following_ids\"], 0, len(entrepreneur[\"following_ids\"]),max_friends_per_user)\n",
    "\n",
    "        for follower_id in random_sample_followers:\n",
    "            find_and_add_all_tweets_of_user(follower_id,unprocessed,processed,keep_track_of_tweets_collection,max_tweets_per_user)\n",
    "        for following_id in random_sample_following:\n",
    "            find_and_add_all_tweets_of_user(following_id,unprocessed,processed,keep_track_of_tweets_collection,max_tweets_per_user)\n",
    "\n",
    "def get_everything_to_a_list(collection, start_from_beggining=False,per_time=200):\n",
    "    flag = False\n",
    "    everything=[]\n",
    "    count = collection.estimated_document_count()\n",
    "    if(count==0):\n",
    "        return []\n",
    "    if(start_from_beggining==True):\n",
    "        count=0\n",
    "    print(count)\n",
    "\n",
    "    while (flag != True):\n",
    "        new_staff = collection.find({}).skip(count).limit(per_time)\n",
    "        if (new_staff == None):\n",
    "            flag = True\n",
    "            break\n",
    "        try:\n",
    "            new_staff = list(new_staff)\n",
    "            print(new_staff)\n",
    "            if(len(new_staff)==0):\n",
    "                flag=True\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "        count = count + len(new_staff)\n",
    "        everything = everything+new_staff\n",
    "    return everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_n_uniqe_random_integers(starting:int=0,final:int =-1,\n",
    "                                n=10):\n",
    "    ans=random.sample(range(starting, final), n)\n",
    "    return ans\n",
    "\n",
    "def find_and_add_all_tweets_of_user(user_id: int, unprocessed, processed,keep_track_of_tweets_collection,max_tweets_per_user=3240):\n",
    "    # Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    temp=keep_track_of_tweets_collection.find_one({\"_id\":user_id})\n",
    "    \n",
    "    if(temp==None):\n",
    "        all_tweet_count=0\n",
    "    else:\n",
    "        all_tweet_count=temp[\"count\"]\n",
    "\n",
    "    # initialize a list to hold all the tweepy Tweets and the tweet data, soon to be the df\n",
    "    tweets_data = []\n",
    "    all_tweets = []\n",
    "\n",
    "    # make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    try:\n",
    "        if(temp==None or temp[\"oldest\"]==-1):\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200)\n",
    "        else:\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200,max_id=temp[\"oldest\"])\n",
    "    except:\n",
    "        print(\"unable to get tweets for user id \",user_id)\n",
    "        return tweets_data\n",
    "    all_tweets = all_tweets + new_tweets\n",
    "    all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "    # process the first tweets and add them to the tweets_data\n",
    "    processed_tweets = process_these_tweets(new_tweets)\n",
    "    tweets_data = tweets_data + processed_tweets\n",
    "    for tweet in new_tweets:\n",
    "        tweet[\"_id\"]=tweet.pop(\"id\")\n",
    "        \n",
    "    oldest=-1\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "    if(len(new_tweets)>0):\n",
    "        oldest = new_tweets[-1][\"_id\"] - 1\n",
    "\n",
    "    try:\n",
    "        if (len(new_tweets) > 0):\n",
    "            unprocessed.insert_many(new_tweets)\n",
    "        if (len(processed_tweets) > 0):\n",
    "            processed.insert_many(processed_tweets)\n",
    "    except:\n",
    "        print(\"i was unable to save evrything to the database\")\n",
    "\n",
    "    # keep grabbing tweets until there are no tweets left to grab\n",
    "    while ((len(new_tweets)>0) and all_tweet_count<max_tweets_per_user) > 0:\n",
    "        print(f\"getting tweets before {oldest}\")\n",
    "        # all subsiquent requests use the max_id param to prevent duplicates\n",
    "        try:\n",
    "            new_tweets = api.user_timeline(user_id=user_id, count=200, max_id=oldest)\n",
    "        except:\n",
    "            print(\"unable to get the rest of the tweets for user id  \", user_id)\n",
    "            break\n",
    "        all_tweets = all_tweets + new_tweets\n",
    "        all_tweet_count = all_tweet_count + len(new_tweets)\n",
    "        # process the tweets and add them to the tweets_data\n",
    "        processed_tweets = process_these_tweets(new_tweets)\n",
    "        tweets_data = tweets_data + processed_tweets\n",
    "        for tweet in new_tweets:\n",
    "            tweet[\"_id\"] = tweet.pop(\"id\")\n",
    "        if(len(new_tweets)>0):\n",
    "            oldest = new_tweets[-1][\"_id\"] - 1\n",
    "        try:\n",
    "            if (len(new_tweets) > 0):\n",
    "                unprocessed.insert_many(new_tweets)\n",
    "                # update the id of the oldest tweet less one\n",
    "            if (len(processed_tweets) > 0):\n",
    "                processed.insert_many(processed_tweets)\n",
    "        except:\n",
    "            print(\"i was unable to save evrything to the database\")\n",
    "\n",
    "        print(f\"...{all_tweet_count} tweets downloaded so far\")\n",
    "    if(temp==None):\n",
    "        q={\n",
    "            \"_id\":user_id,\n",
    "            \"count\":all_tweet_count,\n",
    "            \"oldest\":oldest\n",
    "        }\n",
    "        try:\n",
    "            keep_track_of_tweets_collection.insert_one(q)\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            print(\"attempted dublicate entry\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            keep_track_of_tweets_collection.update_one({\"_id\":user_id},{\"$set\": {\"count\":all_tweet_count,\"oldest\":oldest}})\n",
    "        except:\n",
    "            print(\"i was unable to update the keep_track_of_tweets_collection \")\n",
    "    return tweets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_these_tweets(tweets):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        processed_tweet = process_this_tweet(tweet)\n",
    "        processed_tweets.append(processed_tweet)\n",
    "    return processed_tweets\n",
    "\n",
    "\n",
    "def process_this_tweet(tweet):\n",
    "    status = tweet\n",
    "    status_dict = dict(status)\n",
    "    # ta keys p en eshi apefthias pio kato.\n",
    "    user_id = status_dict[\"user\"][\"id\"]\n",
    "    user = status_dict[\"user\"][\"screen_name\"]\n",
    "    original_text = status_dict[\"text\"]\n",
    "    clean_text = clean_tweet(original_text)\n",
    "    # find polarity and subjectiviy\n",
    "    blob = TextBlob(clean_text)\n",
    "    Sentiment = blob.sentiment\n",
    "    polarity = Sentiment.polarity\n",
    "    subjectivity = Sentiment.subjectivity\n",
    "    # find hashtags\n",
    "    hashtags = []\n",
    "    lod_for_hashtags = status_dict[\"entities\"][\"hashtags\"]\n",
    "    for dic in lod_for_hashtags:\n",
    "        hashtags.append(dic[\"text\"])\n",
    "    # find user_mentions\n",
    "    user_mentions = []\n",
    "    lod_for_user_mentions = status_dict[\"entities\"][\"user_mentions\"]\n",
    "    for dic in lod_for_user_mentions:\n",
    "        user_mentions.append(dic[\"screen_name\"])\n",
    "    # find sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(original_text)\n",
    "    sentiment = vs[\"compound\"]  # mono tuto valo ala maybe ena theli je ta alla 3. tra na dume.\n",
    "    positive = vs[\"pos\"]\n",
    "    negative = vs[\"neg\"]\n",
    "    neutral = vs[\"neu\"]\n",
    "    # find id\n",
    "    inti = status_dict[\"id\"]\n",
    "    is_quote = status_dict[\"is_quote_status\"]\n",
    "    retweet_count = status_dict[\"retweet_count\"]\n",
    "    favorite_count = status_dict[\"favorite_count\"]\n",
    "    is_reply = None\n",
    "    if (status_dict[\"in_reply_to_screen_name\"] != None):\n",
    "        is_reply = True\n",
    "    else:\n",
    "        is_reply = False\n",
    "\n",
    "    # neg=negative, neu=neutral , pos=positive compound vasika\n",
    "    # positive sentiment: compound score >= 0.05\n",
    "    # neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    # negative sentiment: compound score <= -0.05\n",
    "    # -1 (most extreme negative) and +1 (most extreme positive).\n",
    "    # me to for vali ta columns p en mesti lista colums.\n",
    "    ans = {\"_id\": inti,\n",
    "           \"user\": user,\n",
    "           \"user_id\": user_id,\n",
    "           \"original_text\": original_text,\n",
    "           \"clean_text\": clean_text,\n",
    "           \"polarity\": polarity,\n",
    "           \"subjectivity\": subjectivity,\n",
    "           \"hashtag_count\": len(hashtags),\n",
    "           \"user_mention_count\": len(user_mentions),\n",
    "           \"sentiment\": sentiment,\n",
    "           \"is_quote\": is_quote,\n",
    "           \"retweet_count\": retweet_count,\n",
    "           \"favorite_count\": favorite_count,\n",
    "           \"is_reply\": is_reply\n",
    "\n",
    "           }\n",
    "    return ans\n",
    "\n",
    "def process_and_add_tweets_to_database(unpne_tweets_col,processed_tweets_collection,tweets_os_tora=0):\n",
    "    flag=False\n",
    "    count_of_tweets=processed_tweets_collection.count()\n",
    "    while(flag!=True):\n",
    "        unpne_tweets=unpne_tweets_col.find({}).skip(count_of_tweets).limit(200)\n",
    "        if(unpne_tweets==None):\n",
    "            flag=True\n",
    "            break\n",
    "        try:\n",
    "            unpne_tweets=list(unpne_tweets)\n",
    "        except:\n",
    "            continue\n",
    "        count_of_tweets=count_of_tweets+len(unpne_tweets)\n",
    "        processed_tweets=process_these_tweets(unpne_tweets)\n",
    "        try:\n",
    "            processed_tweets_collection.insert_many(processed_tweets)\n",
    "        except:\n",
    "            print(\"i could not add all of them\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    return (matrix[size_x - 1, size_y - 1])\n",
    "\n",
    "def get_current_date():\n",
    "    temp=datetime.today().strftime('%d-%m-%Y')\n",
    "    temp=temp.split(\"-\")\n",
    "    ans_date=(int(temp[0]),int(temp[1]),int(temp[2]))\n",
    "    return ans_date\n",
    "\n",
    "def get_days_so_far(today, then):\n",
    "    d_today=date(today[2],today[1],today[0])\n",
    "    d_then=date(then[2],then[1],then[0])\n",
    "    days=d_today-d_then\n",
    "    return days.days\n",
    "\n",
    "def find_number_of_posts_per_week(number_of_posts, created_at):\n",
    "    then=find_date(created_at)\n",
    "    today=get_current_date()\n",
    "    days_between=get_days_so_far(today,then)\n",
    "    ans=number_of_posts/days_between\n",
    "    return ans\n",
    "\n",
    "import re\n",
    "# Sad Emoticons\n",
    "emoticons_sad = {':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "                 '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('}\n",
    "#HappyEmoticons\n",
    "emoticons_happy = {':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D',\n",
    "                   '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P',\n",
    "                   ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'}\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "def find_a_count(text):\n",
    "    lista=text.split()\n",
    "    count=0\n",
    "    for word in lista:\n",
    "        if (word.startswith(\"@\") and (len(word)>1)):\n",
    "            count=count+1\n",
    "    return count\n",
    "            \n",
    "\n",
    "def removeContractions(text:str)->str:\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    lista= []\n",
    "    keep=True\n",
    "    for w in w_tokenizer.tokenize((text)):\n",
    "        if(w==\"?\"):\n",
    "            keep=False\n",
    "        elif(keep==False):\n",
    "            keep= True\n",
    "        else:\n",
    "            lista.append(w)\n",
    "  #  print(len(lista))\n",
    "    ans=' '.join(lista)\n",
    "    return ans\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    return ' '.join(word.strip(string.punctuation) for word in words.split())\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    lista= []\n",
    "    for w in w_tokenizer.tokenize((text)):\n",
    "         lista.append(lemmatizer.lemmatize(w,get_wordnet_pos(w)))\n",
    "    ans=' '.join(lista)\n",
    "    return ans\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    return set(part[1:] for part in s.split() if part.startswith('#'))\n",
    "\n",
    "def clean_description(dirty: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # gia na eshi ta new lines\n",
    "    tweet = dirty.replace('\\n', ' ').replace('\\r', '')\n",
    "    # Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #     print(\"lower\",tweet)\n",
    "    # removes punctuation\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    #     print(\"punctuation\",tweet)\n",
    "    # remove contractions\n",
    "    tweet = removeContractions(tweet)\n",
    "    #     print(\"contractions\",tweet)\n",
    "    # clean this shit\n",
    "    tweet = p.clean(tweet)\n",
    "    #     print(\"cleaned it\",tweet)\n",
    "    # after tweepy preprocessing the colon symbol left remain after removing mentions\n",
    "\n",
    "    # replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    #     print(\"ascii char\",tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    #     print(tweet)\n",
    "    tweet = re.sub(r'Ä¶', '', tweet)\n",
    "    #     print(tweet)\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    # print(\"emojies\",tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "    # print(\"remove numbers\",tweet)\n",
    "\n",
    "    # lemmatize text\n",
    "    tweet = lemmatize_text(tweet)\n",
    "    # print(\"lem\",tweet)\n",
    "\n",
    "    # token word\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    # remove stopwords\n",
    "    clean = []\n",
    "    w_t = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words:\n",
    "            w_t.append(i)\n",
    "    word_tokens = w_t\n",
    "\n",
    "    #  print(\"stop words\",' '.join(word_tokens))\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons:\n",
    "            clean.append(w)\n",
    "    return ' '.join(clean)\n",
    "\n",
    "\n",
    "\n",
    "def process_description(description:str)->str:\n",
    "\n",
    "    original_text = description\n",
    "    clean_text = clean_description(original_text)\n",
    "    # find polarity and subjectiviy\n",
    "    blob = TextBlob(clean_text)\n",
    "    Sentiment = blob.sentiment\n",
    "    polarity = Sentiment.polarity\n",
    "    subjectivity = Sentiment.subjectivity\n",
    "    # find hashtags\n",
    "    hashtag_count = len(extract_hash_tags(original_text))\n",
    "    # find user_mentions\n",
    "    user_mention_count = find_a_count(original_text)\n",
    "    # find sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(original_text)\n",
    "    sentiment = vs[\"compound\"]  # mono tuto valo ala maybe ena theli je ta alla 3. tra na dume.\n",
    "    positive = vs[\"pos\"]\n",
    "    negative = vs[\"neg\"]\n",
    "    neutral = vs[\"neu\"]\n",
    "\n",
    "    # neg=negative, neu=neutral , pos=positive compound vasika\n",
    "    # positive sentiment: compound score >= 0.05\n",
    "    # neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "    # negative sentiment: compound score <= -0.05\n",
    "    # -1 (most extreme negative) and +1 (most extreme positive).\n",
    "    # me to for vali ta columns p en mesti lista colums.\n",
    "    discreption_dict = {\n",
    "                         \"polarity\": polarity,\n",
    "                         \"subjectivity\": subjectivity,\n",
    "                         \"hashtag_count\": hashtag_count,\n",
    "                         \"user_mention_count\": user_mention_count,\n",
    "                         \"sentiment\": sentiment\n",
    "                         }\n",
    "\n",
    "    return discreption_dict\n",
    "\n",
    "\n",
    "def get_avg_sentiment_of_user(user_id):\n",
    "\n",
    "    all_tweeets_of_user=collection.find({\"user\":{\"id\":user_id}})\n",
    "    count=0\n",
    "    for i,tweet in enumerate(all_tweeets_of_user):\n",
    "        sum_sentiment=sum_sentiment+tweet[\"sentiment\"]\n",
    "        count=i+1\n",
    "\n",
    "    return sum_sentiment/count\n",
    "\n",
    "\n",
    "def find_followers_following_ratio(follower_count, following_count):\n",
    "    return follower_count/following_count\n",
    "\n",
    "\n",
    "\n",
    "def find_date(date_str:str):\n",
    "    months={ 'Jan':1,\n",
    "        \"Feb\":2,\n",
    "        \"Mar\":3,\n",
    "        \"Apr\":4,\n",
    "        \"May\": 5,\n",
    "        \"Jun\":6,\n",
    "        \"Jul\":7,\n",
    "        \"Aug\":8,\n",
    "        \"Sep\":9,\n",
    "        \"Oct\":10,\n",
    "        \"Nov\":11,\n",
    "        \"Dec\":12}\n",
    "    \n",
    "    splitted=date_str.split()\n",
    "    month=months[splitted[1]]\n",
    "    day=splitted[2]\n",
    "    year=splitted[-1]\n",
    "    try:\n",
    "        ans_date=(int(day),month,int(year))\n",
    "    except:\n",
    "        ans_date=None\n",
    "    return ans_date\n",
    "\n",
    "def find_number_of_posts_per_week(number_of_posts, created_at):\n",
    "    then=find_date(created_at)\n",
    "    today=get_current_date()\n",
    "    days_between=get_days_so_far(today,then)\n",
    "    ans=number_of_posts/days_between\n",
    "    return ans\n",
    "\n",
    "\n",
    "def clean_tweet(dirty: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # gia na eshi ta new lines\n",
    "    tweet = dirty.replace('\\n', ' ').replace('\\r', '')\n",
    "    # Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    #     print(\"lower\",tweet)\n",
    "    # removes punctuation\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    #     print(\"punctuation\",tweet)\n",
    "    # remove contractions\n",
    "    tweet = removeContractions(tweet)\n",
    "    #     print(\"contractions\",tweet)\n",
    "    # clean this shit\n",
    "    tweet = p.clean(tweet)\n",
    "    #     print(\"cleaned it\",tweet)\n",
    "    # after tweepy preprocessing the colon symbol left remain after removing mentions\n",
    "\n",
    "    # replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    #     print(\"ascii char\",tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    #     print(tweet)\n",
    "    tweet = re.sub(r'Ä¶', '', tweet)\n",
    "    #     print(tweet)\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    # print(\"emojies\",tweet)\n",
    "    # remove numbers\n",
    "    tweet = re.sub(r\"\\d+\", \"\", tweet)\n",
    "    # print(\"remove numbers\",tweet)\n",
    "\n",
    "    # lemmatize text\n",
    "    tweet = lemmatize_text(tweet)\n",
    "    # print(\"lem\",tweet)\n",
    "\n",
    "    # token word\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    # remove stopwords\n",
    "    clean = []\n",
    "    w_t = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words:\n",
    "            w_t.append(i)\n",
    "    word_tokens = w_t\n",
    "\n",
    "    #  print(\"stop words\",' '.join(word_tokens))\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons:\n",
    "            clean.append(w)\n",
    "    return ' '.join(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"testDB\"]\n",
    "processed = db[\"processed_tweets_friends_and_followers\"]\n",
    "unprocessed = db[\"unprocessed_tweets_friends_and_followers\"]\n",
    "final_sample_collection = db[\"final_sample\"]\n",
    "keep_track_of_tweets_collection=db[\"keep_track_of_tweets\"]\n",
    "\n",
    "entrepreneurs = get_everything_to_a_list(final_sample_collection,True)\n",
    "max_tweets_per_user = 600\n",
    "max_friends_per_user = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entrepreneurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 549623383009136639\n",
      "...15 tweets downloaded so far\n",
      "getting tweets before 10226458115\n",
      "...4 tweets downloaded so far\n",
      "getting tweets before 139986905411485696\n",
      "...3 tweets downloaded so far\n",
      "getting tweets before 800676964386250751\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 611047708120367103\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 553133608848740351\n",
      "...763 tweets downloaded so far\n",
      "getting tweets before 845628955029843968\n",
      "...352 tweets downloaded so far\n",
      "getting tweets before 207094560181075967\n",
      "...451 tweets downloaded so far\n",
      "getting tweets before 186146950310146047\n",
      "...451 tweets downloaded so far\n",
      "getting tweets before 1156099614887546879\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1069883964310007807\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 978516237125931008\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 669491709202423807\n",
      "...2 tweets downloaded so far\n",
      "getting tweets before 625199527704682495\n",
      "...96 tweets downloaded so far\n",
      "getting tweets before 840607789709967360\n",
      "...9 tweets downloaded so far\n",
      "getting tweets before 542303093635813377\n",
      "...1 tweets downloaded so far\n",
      "unable to get tweets for user id  957713538025381888\n",
      "getting tweets before 901647607587414015\n",
      "...49 tweets downloaded so far\n",
      "getting tweets before 668467850047197184\n",
      "...20 tweets downloaded so far\n",
      "getting tweets before 734746022983946239\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 596757690132467711\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 514240302388088833\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 712710201707601919\n",
      "...1 tweets downloaded so far\n",
      "getting tweets before 1266955809612877829\n",
      "...4 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 660552879351029759\n",
      "i was unable to save evrything to the database\n",
      "...586 tweets downloaded so far\n",
      "getting tweets before 401219911105540095\n",
      "...586 tweets downloaded so far\n",
      "unable to get tweets for user id  69863777\n",
      "getting tweets before 914789006180737024\n",
      "...151 tweets downloaded so far\n",
      "getting tweets before 600738373167239167\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 545977475033866239\n",
      "...569 tweets downloaded so far\n",
      "getting tweets before 14687772206\n",
      "...569 tweets downloaded so far\n",
      "getting tweets before 1251182225464233986\n",
      "...4 tweets downloaded so far\n",
      "getting tweets before 697106314263015423\n",
      "...7 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 882233481278898175\n",
      "...20 tweets downloaded so far\n",
      "getting tweets before 661859893335339007\n",
      "...50 tweets downloaded so far\n",
      "getting tweets before 9671795957\n",
      "...58 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 154331730944344063\n",
      "...28 tweets downloaded so far\n",
      "getting tweets before 432695718680481791\n",
      "...172 tweets downloaded so far\n",
      "getting tweets before 436045465834897407\n",
      "...80 tweets downloaded so far\n",
      "unable to get tweets for user id  293986653\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 397228229242540031\n",
      "...66 tweets downloaded so far\n",
      "unable to get tweets for user id  968042001554911232\n",
      "unable to get tweets for user id  126897014\n",
      "getting tweets before 180467185628622847\n",
      "...133 tweets downloaded so far\n",
      "getting tweets before 281810407147073535\n",
      "...145 tweets downloaded so far\n",
      "getting tweets before 983895278473367551\n",
      "...301 tweets downloaded so far\n",
      "getting tweets before 971055074343510015\n",
      "...301 tweets downloaded so far\n",
      "getting tweets before 804770992698560511\n",
      "...6 tweets downloaded so far\n",
      "getting tweets before 436764775994097663\n",
      "...285 tweets downloaded so far\n",
      "getting tweets before 81068190859407359\n",
      "...285 tweets downloaded so far\n",
      "getting tweets before 881022442231156735\n",
      "...3 tweets downloaded so far\n",
      "getting tweets before 14794398244016128\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 28575980890\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1255694805008551935\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1238850043467137023\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1157527358800367615\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1261630597128245249\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1245381925767790592\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 773147952877535232\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 767278760051826687\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1271677072767885311\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1259066254511362049\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283072544451506178\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1282279817648304129\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283480200043429890\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283072766065942527\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1282227885093400575\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1280499218759798785\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1262768674001129472\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1233038441962602495\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1270001440740700159\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1263506599487746048\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1212921501797752831\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 961685737430069247\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1098922888592273407\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 899477938403328000\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1261963103811432447\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1233622927699898367\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1222247733458849791\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1147196015394639874\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 749822318059196416\n",
      "...311 tweets downloaded so far\n",
      "getting tweets before 616549968942239744\n",
      "...311 tweets downloaded so far\n",
      "getting tweets before 1153278124416569343\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1070152224997687297\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1267463785658318852\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1248991415071207424\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1254654528072421375\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1196288351583653887\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1262359700491796479\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1241638277985136644\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1258922158975102976\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1250129115782447108\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1210639604589592575\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1147236664084967423\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 393021994348056575\n",
      "...242 tweets downloaded so far\n",
      "getting tweets before 387254001449717759\n",
      "...242 tweets downloaded so far\n",
      "getting tweets before 1283697884337246207\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283302780921405441\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1151234007167905791\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1126577908246564864\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283520996754903039\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283154232300826623\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283954693614571519\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283728302189039617\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1268737536563666943\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1252874176035078145\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1256155149816360960\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1236312596937990143\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1245830246550712319\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1211810035208216575\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1281105562084601855\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1279325457960194048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...600 tweets downloaded so far\n",
      "getting tweets before 154853255862693887\n",
      "...150 tweets downloaded so far\n",
      "getting tweets before 733496567861141503\n",
      "...370 tweets downloaded so far\n",
      "getting tweets before 462310147046772735\n",
      "...370 tweets downloaded so far\n",
      "getting tweets before 1245382829589983231\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1197214458172465151\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1164988257702240255\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1082374205688213505\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 932251632708046847\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 929985134853369855\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 903152648789999616\n",
      "...595 tweets downloaded so far\n",
      "getting tweets before 902779754314469375\n",
      "...794 tweets downloaded so far\n",
      "getting tweets before 1182688720484478976\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 973260504411058175\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 780813534376538111\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 913195322502815743\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 795672203777847295\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1236154652447535103\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1166120736538288128\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1269531135756824576\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1251825891274973183\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1015374927166181375\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 671086917748989951\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 530413727938379775\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 1265476079310868479\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1251361123690967041\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1223278442222968831\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1278760894575468548\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1276382492979720191\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1270694138187354111\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1273654041638105087\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1257972156324970496\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 203018606798315520\n",
      "...2 tweets downloaded so far\n",
      "getting tweets before 13474430845\n",
      "...19 tweets downloaded so far\n",
      "getting tweets before 1150260511621013503\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1071247950775504895\n",
      "...590 tweets downloaded so far\n",
      "getting tweets before 974807549374615552\n",
      "...787 tweets downloaded so far\n",
      "getting tweets before 1154057889054633984\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1100570210153783295\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1072580029354254335\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1058740232172929027\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 789825454383464447\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1166764690321526783\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1103363858964643844\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 674289338704789503\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 4045705211\n",
      "...457 tweets downloaded so far\n",
      "getting tweets before 278703991\n",
      "...457 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 243256251365076992\n",
      "...236 tweets downloaded so far\n",
      "getting tweets before 1280864189733654527\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1278982374110515201\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1243771043430334464\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1207285138142289919\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1174615400652267519\n",
      "...797 tweets downloaded so far\n",
      "getting tweets before 1209691969292648447\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1193389451361312767\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1173171732427005951\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1279083605763878911\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1275048345073926147\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 514013851210510335\n",
      "...394 tweets downloaded so far\n",
      "getting tweets before 477337103437545472\n",
      "...593 tweets downloaded so far\n",
      "getting tweets before 456272059530629119\n",
      "...791 tweets downloaded so far\n",
      "getting tweets before 1283968425493225471\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283832708796481541\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1176308338696642561\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1168508878239064063\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1111891081162080256\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1014009248483688447\n",
      "...592 tweets downloaded so far\n",
      "getting tweets before 857223356894117887\n",
      "...786 tweets downloaded so far\n",
      "getting tweets before 1103187454864052223\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1035042674489917439\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 585124395305304063\n",
      "...96 tweets downloaded so far\n",
      "getting tweets before 1245116918081523711\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1240791263592378367\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1238324036721049599\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1254283909702881279\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1236874494762815487\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1256526262928957439\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1155131369418182655\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1045595555420930047\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1268952100756615168\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1258360419325898753\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1170586147220647936\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1072998436633333759\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 797100867849777151\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 786956852655427583\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1246034709651455999\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1225372775189102592\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1191326008391032832\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1179441599392075775\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1080896211991838719\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 940587678617047046\n",
      "...600 tweets downloaded so far\n",
      "i was unable to save evrything to the database\n",
      "getting tweets before 806811696194981887\n",
      "i was unable to save evrything to the database\n",
      "...470 tweets downloaded so far\n",
      "getting tweets before 652505791556030464\n",
      "...470 tweets downloaded so far\n",
      "getting tweets before 1272700314618036223\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1263315860183285759\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 11308577381\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 10589508934\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1261045135191597055\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1252183249289281535\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1136963291425775616\n",
      "...395 tweets downloaded so far\n",
      "getting tweets before 1073500606734716927\n",
      "...594 tweets downloaded so far\n",
      "getting tweets before 1035613247162863615\n",
      "...794 tweets downloaded so far\n",
      "getting tweets before 1184426926481362943\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1070175533923594239\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1280379397992742911\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1275357083920039935\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1097621533298704385\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 908748252316602368\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1282342622967689215\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1280718219888730111\n",
      "...600 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1274339337090596863\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1265449720643825663\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1276400014177124354\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1267693902787575807\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1136614501053112319\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 969027215408189440\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1281206357056618496\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1277754138797862911\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1161732473249783814\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1087333995501760513\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1272788123458519076\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1260283041748070400\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1251063198909820929\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1048381696209432576\n",
      "...271 tweets downloaded so far\n",
      "getting tweets before 878330564654137343\n",
      "...271 tweets downloaded so far\n",
      "getting tweets before 912649669402017791\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 718970098887700479\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 527845966766804991\n",
      "...617 tweets downloaded so far\n",
      "getting tweets before 1267692719511658496\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1259387361995022341\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1172119167362711551\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1133308378682413056\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1245213614778150912\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1195185750649688063\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1247950982513717247\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1204439259001884677\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283777001543790593\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1283460602808070143\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1009098542730760192\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 889426724953374720\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1263423527627231231\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1259338901447036927\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 953974510339969026\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 896287288258318335\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 856568848216096767\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1133961029388578820\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 964004129071747071\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 478167959735529472\n",
      "...796 tweets downloaded so far\n",
      "getting tweets before 3752270387\n",
      "...26 tweets downloaded so far\n",
      "getting tweets before 1281476859025412096\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1277993340018003967\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1276056684918693887\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1268748922312364031\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 661830535979732991\n",
      "...385 tweets downloaded so far\n",
      "getting tweets before 476195563512164354\n",
      "...552 tweets downloaded so far\n",
      "getting tweets before 1138321014\n",
      "...552 tweets downloaded so far\n",
      "getting tweets before 1249733059043143679\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1192341038318247938\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1116114564834136063\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 881987651464699903\n",
      "...594 tweets downloaded so far\n",
      "getting tweets before 837095159816826880\n",
      "...791 tweets downloaded so far\n",
      "getting tweets before 1279646929538805762\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1273941285536071682\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1168512428675670015\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1058896148503584767\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1283502817106419718\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1282978196850237439\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1201170762859765759\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1137910830211768320\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1282289071939780608\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1280494843832217602\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1279018877687681023\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1234493412281204735\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1215998649848623103\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1270339611630243839\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 1261166193836208132\n",
      "...595 tweets downloaded so far\n",
      "getting tweets before 1258668834401157120\n",
      "...795 tweets downloaded so far\n",
      "getting tweets before 402152447847972863\n",
      "...3 tweets downloaded so far\n",
      "getting tweets before 1142038690861993983\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1043200358649147398\n",
      "...584 tweets downloaded so far\n",
      "getting tweets before 1015807186461773823\n",
      "...783 tweets downloaded so far\n",
      "getting tweets before 1280097217643196416\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1272860424900231169\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1279616622555062271\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1271726456385662975\n",
      "...600 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "get_all_tweets_from_not_entrepreneurs(entrepreneurs, unprocessed, processed,\n",
    "    final_sample_collection,keep_track_of_tweets_collection,max_tweets_per_user,max_friends_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset staff\n",
    "\n",
    "\n",
    "final_sample_collection.update_many({}, { \"$unset\": {\"tweets_collected_from_followers_and_followings\": \"\"}});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
